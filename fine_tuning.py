# -*- coding: utf-8 -*-
"""(Fine-tuning) Final assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IF366bpAb55kWhDz13vBdbWpcPU2op0R

# Training

## Installation
"""

!pip install sentence-transformers

!pip install pytrec_eval

"""## Imports"""

"""
This examples show how to train a Cross-Encoder for the MS Marco dataset (https://github.com/microsoft/MSMARCO-Passage-Ranking).

The query and the passage are passed simoultanously to a Transformer network. The network then returns
a score between 0 and 1 how relevant the passage is for a given query.

The resulting Cross-Encoder can then be used for passage re-ranking: You retrieve for example 100 passages
for a given query, for example with ElasticSearch, and pass the query+retrieved_passage to the CrossEncoder
for scoring. You sort the results then according to the output of the CrossEncoder.

This gives a significant boost compared to out-of-the-box ElasticSearch / BM25 ranking.
"""
from torch.utils.data import DataLoader
from sentence_transformers import LoggingHandler, util
from sentence_transformers.cross_encoder import CrossEncoder
from sentence_transformers.cross_encoder.evaluation import CERerankingEvaluator
from sentence_transformers import InputExample
from datetime import datetime
import gzip
import os
import tarfile
import tqdm
import logging
from collections import defaultdict
import numpy as np
import sys
import pytrec_eval
logger = logging.getLogger()
logger.setLevel(logging.DEBUG)
logging.basicConfig(format='%(asctime)s - %(message)s',datefmt='%Y-%m-%d %H:%M:%S')

"""## Training preparation

### Initialize hyperparameters (e.g., batch size, etc)

#### To prevent from losing the trained model because of getting disconnected from google colab, we suggest you to store trained model on your google drive. In below we do that by loading google.colab and set the path.
"""

from google.colab import drive
drive.mount('/content/gdrive')
base_path = "./gdrive/MyDrive/cross-encoder-reranker-ir-course-2023/"

!mkdir -p $base_path

#First, we define the transformer model we want to fine-tune

train_batch_size = 32
num_epochs = 1
# We train the network with as a binary label task
# Given [query, passage] is the label 0 = irrelevant or 1 = relevant?
# We use a positive-to-negative ratio: For 1 positive sample (label 1) we include 4 negative samples (label 0)
# in our training setup. For the negative samples, we use the triplets provided by MS Marco that
# specify (query, positive sample, negative sample).
pos_neg_ration = 4

# Maximal number of training samples we want to use
max_train_samples = 5e6 #2e7

"""## Load model (cross-encoder/ms-marco-MiniLM-L-2-v2)"""

#We set num_labels=1, which predicts a continous score between 0 and 1
model_name = 'cross-encoder/ms-marco-MiniLM-L-2-v2'
# model_name = 'microsoft/MiniLM-L12-H384-uncased' #this is for the 5th TASK
# model_name = 'cross-encoder/ms-marco-TinyBERT-L-2-v2'
# model_name = 'distilroberta-base'
model = CrossEncoder(model_name, num_labels=1, max_length=512)
model_save_path = base_path  +'finetuned_models/cross-encoder-'+model_name.replace("/", "-")+'-'+datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

"""## Download MSMARCO data + BM25 initial ranking run file"""

### Now we read the MS Marco dataset
data_folder = 'msmarco-data'
os.makedirs(data_folder, exist_ok=True)


#### Read the corpus files, that contain all the passages. Store them in the corpus dict
corpus = {}
collection_filepath = os.path.join(data_folder, 'collection.tsv')
if not os.path.exists(collection_filepath):
    tar_filepath = os.path.join(data_folder, 'collection.tar.gz')
    if not os.path.exists(tar_filepath):
        logging.info("Download collection.tar.gz")
        util.http_get('https://msmarco.blob.core.windows.net/msmarcoranking/collection.tar.gz', tar_filepath)

    with tarfile.open(tar_filepath, "r:gz") as tar:
        tar.extractall(path=data_folder)

with open(collection_filepath, 'r', encoding='utf8') as fIn:
    for line in fIn:
        pid, passage = line.strip().split("\t")
        corpus[pid] = passage


### Read the train queries, store in queries dict
queries = {}
queries_filepath = os.path.join(data_folder, 'queries.train.tsv')
if not os.path.exists(queries_filepath):
    tar_filepath = os.path.join(data_folder, 'queries.tar.gz')
    if not os.path.exists(tar_filepath):
        logging.info("Download queries.tar.gz")
        util.http_get('https://msmarco.blob.core.windows.net/msmarcoranking/queries.tar.gz', tar_filepath)

    with tarfile.open(tar_filepath, "r:gz") as tar:
        tar.extractall(path=data_folder)


with open(queries_filepath, 'r', encoding='utf8') as fIn:
    for line in fIn:
        qid, query = line.strip().split("\t")
        queries[qid] = query



### Now we create our training & dev data
train_samples = []
dev_samples = {}

# We use 200 random queries from the train set for evaluation during training
# Each query has at least one relevant and up to 200 irrelevant (negative) passages
num_dev_queries = 200
num_max_dev_negatives = 200

# msmarco-qidpidtriples.rnd-shuf.train-eval.tsv.gz and msmarco-qidpidtriples.rnd-shuf.train.tsv.gz is a randomly
# shuffled version of qidpidtriples.train.full.2.tsv.gz from the MS Marco website
# We extracted in the train-eval split 500 random queries that can be used for evaluation during training
train_eval_filepath = os.path.join(data_folder, 'msmarco-qidpidtriples.rnd-shuf.train-eval.tsv.gz')
if not os.path.exists(train_eval_filepath):
    logging.info("Download "+os.path.basename(train_eval_filepath))
    util.http_get('https://sbert.net/datasets/msmarco-qidpidtriples.rnd-shuf.train-eval.tsv.gz', train_eval_filepath)

with gzip.open(train_eval_filepath, 'rt') as fIn:
    for line in fIn:
        qid, pos_id, neg_id = line.strip().split()

        if qid not in dev_samples and len(dev_samples) < num_dev_queries:
            dev_samples[qid] = {'query': queries[qid], 'positive': set(), 'negative': set()}

        if qid in dev_samples:
            dev_samples[qid]['positive'].add(corpus[pos_id])

            if len(dev_samples[qid]['negative']) < num_max_dev_negatives:
                dev_samples[qid]['negative'].add(corpus[neg_id])


# Read our training file
train_filepath = os.path.join(data_folder, 'msmarco-qidpidtriples.rnd-shuf.train.tsv.gz')
if not os.path.exists(train_filepath):
    logging.info("Download "+os.path.basename(train_filepath))
    util.http_get('https://sbert.net/datasets/msmarco-qidpidtriples.rnd-shuf.train.tsv.gz', train_filepath)

cnt = 0
with gzip.open(train_filepath, 'rt') as fIn:
    for line in tqdm.tqdm(fIn, unit_scale=True):
        qid, pos_id, neg_id = line.strip().split()

        if qid in dev_samples:
            continue

        query = queries[qid]
        if (cnt % (pos_neg_ration+1)) == 0:
            passage = corpus[pos_id]
            label = 1
        else:
            passage = corpus[neg_id]
            label = 0

        train_samples.append(InputExample(texts=[query, passage], label=label))
        cnt += 1

        if cnt >= max_train_samples:
            break

"""## Initialize dataloader"""

# We create a DataLoader to load our train samples
train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size)

"""## Initialize CERerankingEvaluator Class
### The CERerankingEvaluator class evaluates the model after every 1k steps of training on the validation set
### Currently, CERerankingEvaluator computes MRR@10 on the valiadion set. You need to change MRR@10 to NDCG@10 for Exercise 4.
###For that, you can download the CERerankingEvaluator class ([link](https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/cross_encoder/evaluation/CERerankingEvaluator.py)) and upload the modified implementation to the brightspace.



"""

# We add an evaluator, which evaluates the performance during training
# It performs a classification task and measures scores like F1 (finding relevant passages) and Average Precision
evaluator = CERerankingEvaluator(dev_samples, name='train-eval')

"""## Train the model
### You can stop the training after one hour by stopping the run
"""

# Train the model
model.fit(train_dataloader=train_dataloader,
          evaluator=evaluator,
          epochs=num_epochs,
          evaluation_steps=1000,
          warmup_steps=5000,
          output_path=model_save_path,
          use_amp=True)

pip install ranx

# import os
# import requests

# # for file in ["content/run1.run", "content/run2.run", "content/run3.run"]:
# for file in ["run1", "run2", "run3"]:

#     os.makedirs("notebooks/data", exist_ok=True)

#     with open(f"notebooks/data/{file}.trec", "w") as f:
#     # with open(f"notebooks/data/{file}.trec", "w") as f:
#         master = f"https://raw.githubusercontent.com/AmenRa/ranx/master/notebooks/data/{file}.trec"
#         master = https://raw.githubusercontent.com/AmenRa/ranx/master/notebooks/data/run_1.trec
#         f.write(requests.get(master).text)

import os
import requests


os.makedirs("notebooks/data", exist_ok=True)

with open(f"notebooks/data/run_1.trec", "w") as f:
  master = "https://raw.githubusercontent.com/AmenRa/ranx/master/notebooks/data/run_1.trec"
  f.write(requests.get(master).text)

with open(f"notebooks/data/run_2.trec", "w") as f:
  master = "https://raw.githubusercontent.com/AmenRa/ranx/master/notebooks/data/run_2.trec"
  f.write(requests.get(master).text)

with open(f"notebooks/data/run_3.trec", "w") as f:
  master = "https://raw.githubusercontent.com/AmenRa/ranx/master/notebooks/data/run_3.trec"
  f.write(requests.get(master).text)

with open(f"notebooks/data/qrels.trec", "w") as f:
  master = "https://raw.githubusercontent.com/AmenRa/ranx/master/notebooks/data/qrels.trec"
  f.write(requests.get(master).text)

from google.colab import drive
drive.mount('/content/drive')

from ranx import Qrels, Run

# Let's load qrels and runs from files
# qrels = Qrels.from_file("notebooks/data/qrels.trec")

run_1 = Run.from_file('notebooks/data/run_1.trec')

run_2 = Run.from_file('notebooks/data/run_2.trec')

run_3 = Run.from_file('notebooks/data/run_3.trec')

qrels = Qrels.from_file('notebooks/data/qrels.trec')

"""TASK 2"""

from ranx import fuse, evaluate

for method in [
    "min",  # Alias for CombMIN
    "max",  # Alias for CombMAX
    "med",  # Alias for CombMED
    "sum",  # Alias for CombSUM
    "anz",  # Alias for CombANZ
    "mnz",  # Alias for CombMNZ
]:
  combined_run = fuse(
      runs=[run_1, run_2, run_3],  # A list of Run instances to fuse
      norm="min-max",       # The normalization strategy to apply before fusion
      method=method,         # The fusion algorithm to use
  )

  # evaluate(qrels, combined_run, ["ndcg@10","recall@100","map@1000"], make_comparable=False)

  print(combined_run.name, evaluate(qrels, combined_run, ["ndcg@10","recall@100","map@1000"], make_comparable=False))

from ranx import fuse, evaluate

for method in [
    # "min",  # Alias for CombMIN
    # "max",  # Alias for CombMAX
    # "med",  # Alias for CombMED
    # "sum",  # Alias for CombSUM
    # "anz",  # Alias for CombANZ
    "mnz",  # Alias for CombMNZ
]:
  combined_run = fuse(
      runs=[run_1, run_2],  # A list of Run instances to fuse
      norm="min-max",       # The normalization strategy to apply before fusion
      method=method,         # The fusion algorithm to use
  )

  # evaluate(qrels, combined_run, ["ndcg@10","recall@100","map@1000"], make_comparable=False)

  print(combined_run.name, evaluate(qrels, combined_run, ["ndcg@10","recall@100","map@1000"], make_comparable=False))

from ranx import fuse, evaluate

for method in [
    # "min",  # Alias for CombMIN
    # "max",  # Alias for CombMAX
    # "med",  # Alias for CombMED
    # "sum",  # Alias for CombSUM
    # "anz",  # Alias for CombANZ
    "mnz",  # Alias for CombMNZ
]:
  combined_run = fuse(
      runs=[run_1, run_3],  # A list of Run instances to fuse
      norm="min-max",       # The normalization strategy to apply before fusion
      method=method,         # The fusion algorithm to use
  )

  # evaluate(qrels, combined_run, ["ndcg@10","recall@100","map@1000"], make_comparable=False)

  print(combined_run.name, evaluate(qrels, combined_run, ["ndcg@10","recall@100","map@1000"], make_comparable=False))

from ranx import fuse, evaluate

for method in [
    # "min",  # Alias for CombMIN
    # "max",  # Alias for CombMAX
    # "med",  # Alias for CombMED
    # "sum",  # Alias for CombSUM
    # "anz",  # Alias for CombANZ
    "mnz",  # Alias for CombMNZ
]:
  combined_run = fuse(
      runs=[ run_2, run_3],  # A list of Run instances to fuse
      norm="min-max",       # The normalization strategy to apply before fusion
      method=method,         # The fusion algorithm to use
  )

  # evaluate(qrels, combined_run, ["ndcg@10","recall@100","map@1000"], make_comparable=False)

  print(combined_run.name, evaluate(qrels, combined_run, ["ndcg@10","recall@100","map@1000"], make_comparable=False))